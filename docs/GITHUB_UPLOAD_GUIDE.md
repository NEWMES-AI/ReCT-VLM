# GitHub ì—…ë¡œë“œ ê°€ì´ë“œ

ReCT-VLM í”„ë¡œì íŠ¸ë¥¼ GitHubì— ì—…ë¡œë“œí•˜ê¸° ìœ„í•œ ìƒì„¸ ê°€ì´ë“œìž…ë‹ˆë‹¤.

## ðŸ“‹ ì‚¬ì „ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. í•„ìˆ˜ íŒŒì¼ í™•ì¸
- [x] README.md
- [x] LICENSE
- [x] requirements.txt
- [x] setup.py
- [x] .gitignore
- [x] CONTRIBUTING.md

### 2. ì½”ë“œ êµ¬ì¡° í™•ì¸
```
ReCT-VLM/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ CONTRIBUTING.md
â”‚
â”œâ”€â”€ rect_vlm/                      # íŒ¨í‚¤ì§€ëª… ë³€ê²½ í•„ìš”
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ configs/
â”œâ”€â”€ scripts/
â”œâ”€â”€ DATA/
â”œâ”€â”€ docs/
â”œâ”€â”€ examples/
â””â”€â”€ tests/
```

## ðŸ”§ ì—…ë¡œë“œ ì „ ìž‘ì—…

### Step 1: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìž¬êµ¬ì„±

í˜„ìž¬ `Method/Vision_Encoder_3D/` êµ¬ì¡°ë¥¼ GitHub repository ë£¨íŠ¸ë¡œ ì´ë™í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# í˜„ìž¬ ìœ„ì¹˜ì—ì„œ
cd /home/work/3D_CT_Foundation_Model/Method/Vision_Encoder_3D/

# íŒ¨í‚¤ì§€ëª… ë³€ê²½ (model â†’ rect_vlm)
mv model rect_vlm

# __init__.py ìƒì„±
cat > rect_vlm/__init__.py << 'EOF'
"""
ReCT-VLM: Reasoning-Enhanced CT Vision-Language Model
"""

__version__ = "0.1.0"

from .model.vision_encoder import ThreeDVisionEncoder
from .model.classification_head import MultiLabelClassifier
from .model.localization_module import LesionLocalizationModule
from .model.report_generator import ReportGenerator
from .model.multi_task_model import VLM3DMultiTask

__all__ = [
    'ThreeDVisionEncoder',
    'MultiLabelClassifier',
    'LesionLocalizationModule',
    'ReportGenerator',
    'VLM3DMultiTask',
]
EOF
```

### Step 2: ë¬¸ì„œ ìž¬ë°°ì¹˜

```bash
# docs ë””ë ‰í† ë¦¬ ìƒì„± ë° ë¬¸ì„œ ì´ë™
mkdir -p docs/images

# ì£¼ìš” ë¬¸ì„œë“¤ì„ docsë¡œ ì´ë™
mv ARCHITECTURE.md docs/
mv TRAINING_PLAN.md docs/TRAINING.md
mv SUB_OBJECTIVE_3_*.md docs/
mv MODULE_TRAINING_DETAILS.md docs/
mv IMPLEMENTATION_SUMMARY.md docs/
mv PROGRESS.md docs/

# READMEëŠ” ë£¨íŠ¸ì— ìœ ì§€
# LICENSE, CONTRIBUTING.mdë„ ë£¨íŠ¸ì— ìœ ì§€
```

### Step 3: ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸ ì •ë¦¬

```bash
# DATA ë””ë ‰í† ë¦¬ ì •ë¦¬
cd DATA/

# README ìƒì„±
cat > README.md << 'EOF'
# Dataset Preparation

## CT-RATE Dataset

### Download
\`\`\`bash
python download_dataset.py
\`\`\`

### Prepare for Training
\`\`\`bash
python prepare_ctrate_for_medsam2.py \
    --volume-dir ./CT-RATE/lung_nodule/volume \
    --mask-dir ./CT-RATE/lung_nodule/masks \
    --output-dir ./CT-RATE/lung_nodule_medsam2 \
    --split all
\`\`\`

See [detailed instructions](../docs/DATA_PREPARATION.md).
EOF
```

### Step 4: ê°€ì¤‘ì¹˜ ê´€ë¦¬

ì‹¤ì œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ëŠ” ìš©ëŸ‰ì´ í¬ë¯€ë¡œ HuggingFace Hubì— ì—…ë¡œë“œí•˜ê³ , GitHubì—ëŠ” ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ë§Œ í¬í•¨í•©ë‹ˆë‹¤.

```bash
# scripts ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p scripts

# ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > scripts/download_weights.py << 'EOF'
#!/usr/bin/env python3
"""
Download pre-trained weights from HuggingFace Hub
"""

import argparse
from huggingface_hub import hf_hub_download
import os

REPO_ID = "NEWMES-AI/ReCT-VLM"

WEIGHT_FILES = {
    "vision-encoder": "checkpoints/vision_encoder.pt",
    "classification": "checkpoints/classification_head.pt",
    "localization": "checkpoints/localization_module.pt",
    "full-model": "checkpoints/full_model.pt",
}

def download_weights(component: str, output_dir: str = "./checkpoints"):
    """Download specific component weights."""
    os.makedirs(output_dir, exist_ok=True)

    if component == "all":
        for comp, filename in WEIGHT_FILES.items():
            print(f"Downloading {comp}...")
            hf_hub_download(
                repo_id=REPO_ID,
                filename=filename,
                local_dir=output_dir
            )
    else:
        filename = WEIGHT_FILES.get(component)
        if filename:
            print(f"Downloading {component}...")
            hf_hub_download(
                repo_id=REPO_ID,
                filename=filename,
                local_dir=output_dir
            )
        else:
            print(f"Unknown component: {component}")
            print(f"Available: {list(WEIGHT_FILES.keys())}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--component", choices=["all", "vision-encoder", "classification", "localization", "full-model"], default="all")
    parser.add_argument("--output-dir", default="./checkpoints")
    args = parser.parse_args()

    download_weights(args.component, args.output_dir)
EOF

chmod +x scripts/download_weights.py
```

### Step 5: í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±

```bash
# tests ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p tests

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
cat > tests/test_vision_encoder.py << 'EOF'
import pytest
import torch
from rect_vlm.model.vision_encoder import ThreeDVisionEncoder

def test_vision_encoder_forward():
    model = ThreeDVisionEncoder(
        in_channels=1,
        embed_dim=768,
        depth=12,
        num_heads=12
    )

    x = torch.randn(1, 1, 64, 512, 512)
    seg_mask = torch.randint(0, 20, (1, 64, 512, 512))

    outputs = model(x, seg_mask)

    assert outputs['global_features'].shape == (1, 768)
    assert outputs['local_features'].shape[0] == 1
    assert outputs['region_features'].shape == (1, 20, 768)

if __name__ == "__main__":
    test_vision_encoder_forward()
    print("âœ“ Vision encoder test passed")
EOF

# __init__.py
touch tests/__init__.py
```

## ðŸ“¤ GitHub ì—…ë¡œë“œ ì ˆì°¨

### Step 1: Git ì´ˆê¸°í™”

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
cd /home/work/3D_CT_Foundation_Model/Method/Vision_Encoder_3D/

# Git ì´ˆê¸°í™”
git init

# .gitignore ì ìš©
git add .gitignore
git commit -m "chore: add .gitignore"
```

### Step 2: ì´ˆê¸° ì»¤ë°‹

```bash
# ëª¨ë“  íŒŒì¼ ì¶”ê°€
git add .

# ì»¤ë°‹ (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì œì™¸ë¨)
git commit -m "Initial commit: ReCT-VLM implementation

- Add 3D Vision Encoder with slice-aware attention
- Add multi-label classification head with BioBERT
- Add 3-stage lesion localization module
- Add LLM-based report generator with LoRA
- Add multi-task learning integration
- Add training infrastructure and metrics
- Add comprehensive documentation"
```

### Step 3: Remote Repository ì—°ê²°

```bash
# GitHub repository ì—°ê²°
git remote add origin https://github.com/NEWMES-AI/ReCT-VLM.git

# ë¸Œëžœì¹˜ í™•ì¸
git branch -M main
```

### Step 4: Push to GitHub

```bash
# ì²« ë²ˆì§¸ push
git push -u origin main
```

## ðŸ” ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ (HuggingFace)

### ê°€ì¤‘ì¹˜ ì—…ë¡œë“œ (í•™ìŠµ ì™„ë£Œ í›„)

```bash
# HuggingFace Hub ì„¤ì¹˜
pip install huggingface_hub

# ë¡œê·¸ì¸
huggingface-cli login

# Repository ìƒì„± (ì›¹ì—ì„œ ë¨¼ì € ìƒì„± ê¶Œìž¥)
# https://huggingface.co/new

# ê°€ì¤‘ì¹˜ ì—…ë¡œë“œ
python << 'EOF'
from huggingface_hub import HfApi

api = HfApi()

# Upload checkpoints
api.upload_file(
    path_or_fileobj="./checkpoints/full_model.pt",
    path_in_repo="checkpoints/full_model.pt",
    repo_id="NEWMES-AI/ReCT-VLM",
    repo_type="model"
)

# Upload configuration
api.upload_file(
    path_or_fileobj="./configs/config_large.yaml",
    path_in_repo="configs/config_large.yaml",
    repo_id="NEWMES-AI/ReCT-VLM",
    repo_type="model"
)
EOF
```

## ðŸ“Š ë°ì´í„°ì…‹ ì²˜ë¦¬

### CT-RATE ë°ì´í„°ì…‹

CT-RATEëŠ” HuggingFace Datasetsì— ì´ë¯¸ í˜¸ìŠ¤íŒ…ë˜ì–´ ìžˆìœ¼ë¯€ë¡œ, READMEì— ë‹¤ìš´ë¡œë“œ ë°©ë²•ë§Œ ëª…ì‹œ:

```markdown
## Dataset

We use the [CT-RATE dataset](https://huggingface.co/datasets/ibrahimhamamci/CT-RATE).

Download instructions in [DATA/README.md](DATA/README.md).
```

### OmniAbnorm CoT ë°ì´í„°ì…‹

ìƒì„±ëœ CoT ë°ì´í„°ë¥¼ HuggingFace Datasetsë¡œ ê³µìœ :

```bash
# dataset_dict ìƒì„± ë° ì—…ë¡œë“œ
python << 'EOF'
from datasets import Dataset, DatasetDict
import json

# Load generated CoT data
with open("DATA/OmniAbnorm/cot_data.json") as f:
    data = json.load(f)

# Create dataset
dataset = Dataset.from_dict(data)

# Upload to HuggingFace
dataset.push_to_hub("NEWMES-AI/OmniAbnorm-CoT")
EOF
```

## âœ… ì—…ë¡œë“œ í›„ í™•ì¸ ì‚¬í•­

### GitHubì—ì„œ í™•ì¸

1. [ ] README.mdê°€ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë˜ëŠ”ì§€
2. [ ] ë¼ì´ì„ ìŠ¤ê°€ ìžë™ ì¸ì‹ë˜ëŠ”ì§€
3. [ ] .gitignoreê°€ ìž‘ë™í•˜ëŠ”ì§€ (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì œì™¸)
4. [ ] ëª¨ë“  ë¬¸ì„œê°€ ì •ìƒì ìœ¼ë¡œ í‘œì‹œë˜ëŠ”ì§€
5. [ ] ì´ë¯¸ì§€ ë§í¬ê°€ ìž‘ë™í•˜ëŠ”ì§€

### ì„¤ì • í™•ì¸

1. [ ] Repository ì„¤ì •
   - Description ì¶”ê°€
   - Topics ì¶”ê°€ (medical-imaging, computer-vision, vision-language-model, pytorch, transformers)
   - Website ì¶”ê°€ (ìžˆë‹¤ë©´)

2. [ ] GitHub Actions ì„¤ì • (Optional)
   - CI/CD íŒŒì´í”„ë¼ì¸
   - ìžë™ í…ŒìŠ¤íŠ¸
   - ë¬¸ì„œ ë¹Œë“œ

3. [ ] Issues í…œí”Œë¦¿ ìƒì„±
   - Bug report
   - Feature request

## ðŸ“ ì¶”ê°€ ìž‘ì—… (Optional)

### GitHub Pages ì„¤ì •

```bash
# gh-pages ë¸Œëžœì¹˜ ìƒì„±
git checkout --orphan gh-pages
git rm -rf .

# ë¬¸ì„œ ì‚¬ì´íŠ¸ ìƒì„± (Sphinx or MkDocs)
# ...

git add .
git commit -m "docs: initialize GitHub Pages"
git push origin gh-pages
```

### Releases ìƒì„±

```bash
# Tag ìƒì„±
git tag -a v0.1.0 -m "Initial release: ReCT-VLM v0.1.0"
git push origin v0.1.0

# GitHubì—ì„œ Release ìƒì„±
# - Release notes ìž‘ì„±
# - Pre-trained weights ë§í¬ ì¶”ê°€
# - Changelog í¬í•¨
```

### Badges ì¶”ê°€

README.md ìƒë‹¨ì— ì¶”ê°€í•  badges:
- Paper link (arXiv)
- License
- Python version
- PyTorch version
- CI status
- Code coverage
- Downloads

## ðŸš¨ ì£¼ì˜ì‚¬í•­

### ì ˆëŒ€ ì—…ë¡œë“œí•˜ë©´ ì•ˆ ë˜ëŠ” ê²ƒ

1. **ëŒ€ìš©ëŸ‰ íŒŒì¼**
   - ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (*.pt, *.pth)
   - ë°ì´í„°ì…‹ íŒŒì¼ (*.nii.gz, *.npz)
   - ì‹¤í—˜ ê²°ê³¼ íŒŒì¼

2. **ë¯¼ê° ì •ë³´**
   - API keys
   - HuggingFace tokens
   - ê°œì¸ ì •ë³´

3. **ìž„ì‹œ íŒŒì¼**
   - ìºì‹œ íŒŒì¼
   - ë¡œê·¸ íŒŒì¼
   - __pycache__

### Git LFS ì‚¬ìš© (Optional)

ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ GitHubì— ì˜¬ë ¤ì•¼ í•œë‹¤ë©´ Git LFS ì‚¬ìš©:

```bash
# Git LFS ì„¤ì¹˜
git lfs install

# ì¶”ì í•  íŒŒì¼ íƒ€ìž… ì§€ì •
git lfs track "*.pt"
git lfs track "*.pth"

# .gitattributes ì¶”ê°€
git add .gitattributes
git commit -m "chore: configure Git LFS"
```

í•˜ì§€ë§Œ **HuggingFace Hub ì‚¬ìš©ì„ ê¶Œìž¥**í•©ë‹ˆë‹¤.

## ðŸ“® ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì—…ë¡œë“œ ì „ ìµœì¢… í™•ì¸:

- [ ] ëª¨ë“  ì½”ë“œê°€ ì •ìƒ ìž‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
- [ ] README.mdê°€ ì™„ì„±ë˜ì—ˆëŠ”ì§€
- [ ] LICENSEê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€
- [ ] .gitignoreê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€
- [ ] ë¯¼ê° ì •ë³´ê°€ ì œê±°ë˜ì—ˆëŠ”ì§€
- [ ] ë¬¸ì„œê°€ ì™„ì„±ë˜ì—ˆëŠ”ì§€
- [ ] ì˜ˆì œ ì½”ë“œê°€ ìž‘ë™í•˜ëŠ”ì§€
- [ ] ì„¤ì¹˜ ê°€ì´ë“œê°€ ì •í™•í•œì§€

## ðŸŽ‰ ì™„ë£Œ!

ëª¨ë“  ì ˆì°¨ë¥¼ ì™„ë£Œí–ˆë‹¤ë©´:

1. GitHub repository: https://github.com/NEWMES-AI/ReCT-VLM
2. HuggingFace models: https://huggingface.co/NEWMES-AI
3. Documentation: GitHub Pages or ReadTheDocs

**ë‹¤ìŒ ë‹¨ê³„**:
- CI/CD ì„¤ì •
- ë¬¸ì„œ ì‚¬ì´íŠ¸ êµ¬ì¶•
- ì»¤ë®¤ë‹ˆí‹° ê´€ë¦¬
- Issue ëŒ€ì‘
- ì§€ì†ì  ì—…ë°ì´íŠ¸
