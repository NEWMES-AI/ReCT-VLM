# 3D Vision Encoder í›ˆë ¨ ê³„íšì„œ

## ğŸ“‹ ëª¨ë¸ ì •ë³´

### AI ëª¨ë¸ëª…
**VLM3D-CT-Encoder** (3D Vision-Language Model for CT with Chain-of-Thought Alignment)

### ì—°êµ¬ ë‚´ìš©
**ì œëª©**: "Reasoning-Aligned 3D Vision Encoder for CT Image Understanding with Anatomical Context Integration"

**í•µì‹¬ ê¸°ì—¬**:
1. 3D CT volumeì— íŠ¹í™”ëœ native 3D vision encoder
2. Slice-aware attention mechanismìœ¼ë¡œ zì¶• ì—°ì†ì„± ë³´ì¡´
3. Segmentation-guided region-aware attentionìœ¼ë¡œ í•´ë¶€í•™ì  context í†µí•©
4. Chain-of-Thought (CoT) reasoningê³¼ ì •ë ¬ëœ multi-granular representation learning
5. Cross-modal contrastive learningìœ¼ë¡œ visual-text alignment

**ì ìš© ë¶„ì•¼**: VLM3D Challenge (MICCAI 2025) - CT ì˜ìƒ ì´í•´, ë³‘ë³€ ë¶„ë¥˜, ìœ„ì¹˜ ì‹ë³„, íŒë…ë¬¸ ìƒì„±

---

## ğŸ“Š í›ˆë ¨ ë°ì´í„° êµ¬ì„±

### 1. CT-RATE Dataset

#### ë°ì´í„° ìœ í˜•
- **CT Volumes**: 3D chest CT scans
- **Segmentation Masks**: MedSAM2ë¡œ ìƒì„±ëœ í•´ë¶€í•™ì  êµ¬ì¡° ë§ˆìŠ¤í¬
- **CoT Text**: Question-Answer í˜•ì‹ì˜ reasoning

#### íŒŒì¼ í¬ë§·
```
Volume:
  â€¢ Format: .nii.gz (NIfTI compressed)
  â€¢ Shape: (D, H, W) - variable depth, 512Ã—512 typical
  â€¢ Data type: int16 (HU values)
  â€¢ Windowing: Lung window (center=40, width=400)

Mask:
  â€¢ Format: .nii.gz
  â€¢ Shape: Same as volume
  â€¢ Data type: uint8 (region IDs 0-20)

CoT Text:
  â€¢ Format: .json
  â€¢ Fields: volumename, question, answer
  â€¢ Encoding: UTF-8
```

#### í•™ìŠµ ë°ì´í„° ìˆ˜
```
Train split: 21,907 cases
  â€¢ With CoT reasoning: 21,907 cases
  â€¢ Unique volumes: 21,907

Validation split: ì˜ˆì • (~2,000 cases, 10% split)
```

#### íŒŒì¼ ìš©ëŸ‰
```
Per Volume:
  â€¢ CT .nii.gz: ~5-15 MB (ì••ì¶•)
  â€¢ Mask .nii.gz: ~1-3 MB (ì••ì¶•)
  â€¢ CoT .json: ~1-2 KB

Total Dataset Size:
  â€¢ CT volumes: ~219 GB (21,907 Ã— 10 MB avg)
  â€¢ Masks: ~44 GB (21,907 Ã— 2 MB avg)
  â€¢ CoT text: ~44 MB (21,907 Ã— 2 KB avg)
  â€¢ Total: ~263 GB
```

### 2. OmniAbnorm-CT-14K Dataset

#### ë°ì´í„° ìœ í˜•
- **CT Slices**: 2D axial CT slices (JPEG)
- **Segmentation Masks**: Lesion masks (JPEG)
- **CoT Text**: Multi-step reasoning generated by Llama-3.1-70B

#### íŒŒì¼ í¬ë§·
```
Image:
  â€¢ Format: .jpg
  â€¢ Shape: Variable (typically 512Ã—512 - 630Ã—626)
  â€¢ Data type: uint8 RGB (grayscale CT)
  â€¢ Normalization: [0, 255]

Mask:
  â€¢ Format: .jpg
  â€¢ Shape: Same as image
  â€¢ Data type: uint8 (binary mask or multi-class)

CoT Text:
  â€¢ Format: .json (batch files)
  â€¢ Fields: case_id, region, category, cot_reasoning[]
  â€¢ Structure: Multi-step with anatomical entities
```

#### í•™ìŠµ ë°ì´í„° ìˆ˜
```
Current (ë°±ê·¸ë¼ìš´ë“œ ìƒì„± ì¤‘):
  â€¢ Generated: 1,315 cases (13%)
  â€¢ Target: 10,117 cases (100%)
  â€¢ Expected completion: ~67 hours

Final:
  â€¢ Train: ~9,100 cases (90%)
  â€¢ Val: ~1,017 cases (10%)
```

#### íŒŒì¼ ìš©ëŸ‰
```
Per Case:
  â€¢ Image .jpg: ~50-100 KB
  â€¢ Mask .jpg: ~10-30 KB
  â€¢ CoT .json: ~5-10 KB

Total Dataset Size:
  â€¢ Images: ~770 MB (10,117 Ã— 76 KB avg)
  â€¢ Masks: ~200 MB (10,117 Ã— 20 KB avg)
  â€¢ CoT text: ~65 MB (aggregated)
  â€¢ Total: ~1 GB
```

### 3. í†µí•© ë°ì´í„°ì…‹

#### ì´ í›ˆë ¨ ë°ì´í„°
```
Combined Training Set:
  â€¢ CT-RATE: 21,907 volumes (3D)
  â€¢ OmniAbnorm: ~9,100 slices (2D, ì¼ë¶€ 3D ì¬êµ¬ì„± ê°€ëŠ¥)
  â€¢ Total unique cases: ~31,000+

Total Size: ~264 GB
```

---

## âš™ï¸ í›ˆë ¨ ì„¤ì •

### Epoch ì„¤ì •

#### Pre-training Phase (CT-RATE only)
```yaml
Epochs: 50
Batch size: 8 per GPU (16 total with 2 GPUs)
Effective batch size: 16
Gradient accumulation steps: 2
Effective batch size: 32

Iterations per epoch: 21,907 / 32 = 685 iterations
Total iterations: 50 Ã— 685 = 34,250 iterations
```

#### Fine-tuning Phase (CT-RATE + OmniAbnorm)
```yaml
Epochs: 20
Batch size: 16 per GPU (32 total)
Effective batch size: 32

Iterations per epoch: 31,007 / 32 = 970 iterations
Total iterations: 20 Ã— 970 = 19,400 iterations
```

#### Total Training
```
Pre-training: 50 epochs
Fine-tuning: 20 epochs
Total: 70 epochs
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

```yaml
Model:
  embed_dim: 768
  depth: 12
  num_heads: 12
  num_regions: 20
  patch_size: [4, 16, 16]
  mlp_ratio: 4.0
  dropout: 0.1
  attn_dropout: 0.1

Optimizer:
  type: AdamW
  learning_rate: 1e-4
  weight_decay: 0.05
  betas: [0.9, 0.999]
  eps: 1e-8

LR Schedule:
  type: cosine
  warmup_epochs: 5
  min_lr: 1e-6

Training:
  precision: mixed (fp16)
  gradient_clip: 1.0
  gradient_checkpointing: true

Loss:
  contrastive_weight: 1.0
  region_alignment_weight: 0.5
  temperature: 0.07
```

---

## ğŸ”„ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ êµ¬ì„±

### Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Pipeline Flow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] Data Loading & Preprocessing
    â†“
[2] Model Forward Pass (Vision + Text)
    â†“
[3] Feature Extraction (Multi-granular)
    â†“
[4] Loss Computation (Multi-level Contrastive)
    â†“
[5] Backward Pass & Optimization
    â†“
[6] Logging & Checkpointing
```

### 1. Data Loading & Preprocessing

```python
"""
DataLoader Pipeline
"""

class CTRATEDataset(Dataset):
    """CT-RATE Dataset with CoT reasoning"""

    def __init__(self, data_dir, cot_json, transform=None):
        self.volumes = load_volume_paths(data_dir)
        self.cot_data = load_json(cot_json)
        self.transform = transform

    def __getitem__(self, idx):
        # Load CT volume
        volume = load_nifti(self.volumes[idx])  # (D, H, W)

        # Apply windowing (lung window)
        volume = apply_window(volume, center=40, width=400)

        # Resize to fixed shape
        volume = resize_3d(volume, target_size=(64, 512, 512))

        # Normalize to [0, 1]
        volume = (volume - volume.min()) / (volume.max() - volume.min())

        # Load segmentation mask
        mask = load_nifti_mask(self.volumes[idx].replace('volume', 'mask'))
        mask = resize_3d(mask, target_size=(64, 512, 512), interpolation='nearest')

        # Load CoT text
        cot_text = self.cot_data[self.volumes[idx]]
        question = cot_text['question']
        answer = cot_text['answer']

        # Data augmentation (training only)
        if self.transform:
            volume, mask = self.transform(volume, mask)

        return {
            'volume': torch.FloatTensor(volume).unsqueeze(0),  # (1, D, H, W)
            'mask': torch.LongTensor(mask).unsqueeze(0),       # (1, D, H, W)
            'question': question,
            'answer': answer,
            'case_id': self.volumes[idx]
        }


class OmniAbnormDataset(Dataset):
    """OmniAbnorm Dataset with generated CoT"""

    def __getitem__(self, idx):
        # Load 2D slice
        image = load_jpeg(self.images[idx])  # (H, W)

        # Convert to pseudo-3D (stack adjacent slices)
        volume = stack_adjacent_slices(image, num_slices=4)  # (4, H, W)

        # Resize
        volume = resize_3d(volume, target_size=(4, 512, 512))

        # Load mask
        mask = load_jpeg_mask(self.masks[idx])
        mask = resize_2d(mask, target_size=(512, 512))
        mask = np.stack([mask] * 4)  # (4, 512, 512)

        # Load CoT
        cot_data = self.cot_data[idx]
        cot_steps = extract_cot_steps(cot_data['cot_reasoning'])

        return {
            'volume': torch.FloatTensor(volume).unsqueeze(0),
            'mask': torch.LongTensor(mask).unsqueeze(0),
            'cot_steps': cot_steps,
            'case_id': cot_data['case_id']
        }


# Data Augmentation
train_transform = Compose([
    RandomFlip3D(p=0.5),
    RandomRotation3D(degrees=10, p=0.3),
    RandomIntensityShift(shift_range=0.1, p=0.3),
    RandomNoise(noise_std=0.01, p=0.2)
])


# DataLoader
train_loader = DataLoader(
    CTRATEDataset(data_dir, cot_json, transform=train_transform),
    batch_size=8,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2
)
```

### 2. Model Forward Pass

```python
"""
Model Forward Pass
"""

# Vision Encoder
vision_outputs = vision_encoder(
    ct_volume=batch['volume'],      # (B, 1, 64, 512, 512)
    segmentation_mask=batch['mask']  # (B, 1, 64, 512, 512)
)

# Extract features
global_features = vision_outputs['global_features']    # (B, 768)
local_features = vision_outputs['local_features']      # (B, 16384, 768)
region_features = vision_outputs['region_features']    # (B, 20, 768)


# Text Encoder (BioBERT)
text_outputs = text_encoder(
    input_ids=batch['input_ids'],           # (B, max_len)
    attention_mask=batch['attention_mask']   # (B, max_len)
)

# Extract text features
text_global = text_outputs['pooler_output']  # (B, 768)
text_tokens = text_outputs['last_hidden_state']  # (B, max_len, 768)


# Project to shared embedding space
vision_global_proj = vision_encoder.global_proj(global_features)  # (B, 768)
text_global_proj = text_encoder.projection(text_global)           # (B, 768)

# Normalize
vision_global_norm = F.normalize(vision_global_proj, dim=-1)
text_global_norm = F.normalize(text_global_proj, dim=-1)
```

### 3. Loss Computation

```python
"""
Multi-level Contrastive Loss
"""

# 1. Global-level contrastive loss
logits_vision_to_text = torch.matmul(
    vision_global_norm, text_global_norm.T
) / temperature  # (B, B)

logits_text_to_vision = logits_vision_to_text.T

labels = torch.arange(B).to(device)

loss_v2t = F.cross_entropy(logits_vision_to_text, labels)
loss_t2v = F.cross_entropy(logits_text_to_vision, labels)

loss_global = (loss_v2t + loss_t2v) / 2


# 2. Region-level alignment loss
# Match region features with anatomical mentions in text
loss_region = region_alignment_loss(
    region_features,     # (B, 20, 768)
    text_tokens,         # (B, max_len, 768)
    region_mentions      # (B, 20) - binary mask of mentioned regions
)


# 3. Total loss
total_loss = (
    contrastive_weight * loss_global +
    region_alignment_weight * loss_region
)
```

### 4. Training Loop

```python
"""
Training Loop (Simplified)
"""

for epoch in range(num_epochs):
    model.train()

    for batch_idx, batch in enumerate(train_loader):
        # Move to GPU
        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                 for k, v in batch.items()}

        # Forward pass
        with torch.cuda.amp.autocast(enabled=use_fp16):
            vision_outputs = vision_encoder(batch['volume'], batch['mask'])
            text_outputs = text_encoder(batch['input_ids'], batch['attention_mask'])

            # Compute loss
            loss = compute_loss(vision_outputs, text_outputs)

        # Backward pass
        scaler.scale(loss).backward()

        # Gradient clipping
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Optimizer step
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()

        # Logging
        if batch_idx % log_interval == 0:
            log_metrics(epoch, batch_idx, loss)

    # Validation
    if epoch % val_interval == 0:
        val_metrics = validate(val_loader)
        save_checkpoint(epoch, val_metrics)

    # LR scheduler step
    scheduler.step()
```

---

## â±ï¸ ì˜ˆìƒ í›ˆë ¨ ì‹œê°„

### í•˜ë“œì›¨ì–´ ì‚¬ì–‘
```
GPU: 2Ã— NVIDIA H200 (140 GB VRAM each)
CPU: 64 cores
RAM: 512 GB
Storage: NVMe SSD (7 GB/s read)
```

### ì‹œê°„ ì¶”ì •

#### Per-iteration Timing
```
Forward pass (vision encoder):
  â€¢ Batch size 8, volume (64, 512, 512)
  â€¢ Time: ~300 ms

Forward pass (text encoder):
  â€¢ Batch size 8, seq_len 512
  â€¢ Time: ~50 ms

Loss computation: ~10 ms
Backward pass: ~400 ms
Optimizer step: ~50 ms

Total per iteration: ~810 ms â‰ˆ 0.81 seconds
```

#### Pre-training (50 epochs)
```
Iterations per epoch: 685
Time per epoch: 685 Ã— 0.81 sec = 555 sec â‰ˆ 9.2 minutes

Total pre-training time:
  â€¢ 50 epochs Ã— 9.2 min = 460 minutes
  â€¢ â‰ˆ 7.7 hours
```

#### Fine-tuning (20 epochs)
```
Iterations per epoch: 970
Time per epoch: 970 Ã— 0.81 sec = 786 sec â‰ˆ 13.1 minutes

Total fine-tuning time:
  â€¢ 20 epochs Ã— 13.1 min = 262 minutes
  â€¢ â‰ˆ 4.4 hours
```

#### Total Training Time
```
Pre-training:  7.7 hours
Fine-tuning:   4.4 hours
Total:         12.1 hours

With validation, logging, checkpoint saving:
Estimated total: ~15-18 hours
```

### ì‹œê°„ ë¶„í•´ (Total 18 hours)

| Stage | Duration | % |
|-------|----------|---|
| Data loading | 1 hour | 5.6% |
| Forward pass (vision) | 8 hours | 44.4% |
| Forward pass (text) | 1.5 hours | 8.3% |
| Loss computation | 0.5 hour | 2.8% |
| Backward pass | 6 hours | 33.3% |
| Validation | 0.5 hour | 2.8% |
| Checkpointing | 0.5 hour | 2.8% |
| **Total** | **18 hours** | **100%** |

---

## ğŸ’¾ ì €ì¥ ê³µê°„ ìš”êµ¬ì‚¬í•­

### Checkpoint ì €ì¥
```
Per checkpoint:
  â€¢ Model weights: ~350 MB (88M params Ã— 4 bytes)
  â€¢ Optimizer state: ~700 MB (AdamW = 2Ã— model size)
  â€¢ Scheduler state: ~1 MB
  â€¢ Total per checkpoint: ~1.05 GB

Checkpoints:
  â€¢ Save every 5 epochs: 14 checkpoints (pre-train + fine-tune)
  â€¢ Keep best 3 + last 2: 5 checkpoints
  â€¢ Total: ~5.3 GB
```

### Logs & Metrics
```
TensorBoard logs: ~500 MB
Training logs: ~100 MB
Evaluation results: ~50 MB
Total: ~650 MB
```

### Total Storage for Training
```
Dataset: ~264 GB (read-only)
Checkpoints: ~5.3 GB
Logs: ~650 MB
Temporary: ~10 GB (batch preprocessing)
Total: ~280 GB
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### Baseline (Before Training)
```
Random initialization:
  â€¢ Imageâ†’Text Retrieval R@5: ~5-10%
  â€¢ Textâ†’Image Retrieval R@5: ~5-10%
```

### After Pre-training (50 epochs on CT-RATE)
```
Expected performance:
  â€¢ Imageâ†’Text Retrieval R@5: ~40-50%
  â€¢ Textâ†’Image Retrieval R@5: ~40-50%
  â€¢ Zero-shot classification: ~30-40% accuracy
```

### After Fine-tuning (+ OmniAbnorm)
```
Expected performance:
  â€¢ Imageâ†’Text Retrieval R@5: ~55-65%
  â€¢ Textâ†’Image Retrieval R@5: ~55-65%
  â€¢ Zero-shot classification: ~45-55% accuracy
  â€¢ Region-text alignment: ~70-80% accuracy
```

---

## ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´

### 1. Pre-training
```bash
# Multi-GPU training with DDP
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    --master_port=29500 \
    training/train.py \
    --config configs/pretrain_ctrate.yaml \
    --output_dir exp_log/pretrain \
    --num_epochs 50 \
    --batch_size 8 \
    --lr 1e-4 \
    --fp16
```

### 2. Fine-tuning
```bash
python -m torch.distributed.launch \
    --nproc_per_node=2 \
    training/train.py \
    --config configs/finetune_combined.yaml \
    --output_dir exp_log/finetune \
    --num_epochs 20 \
    --batch_size 16 \
    --lr 5e-5 \
    --fp16 \
    --resume exp_log/pretrain/checkpoint_best.pth
```

### 3. Evaluation
```bash
python evaluation/retrieval.py \
    --checkpoint exp_log/finetune/checkpoint_best.pth \
    --data_dir /path/to/test/data \
    --output_dir exp_log/eval_results
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### TensorBoard ì‹¤í–‰
```bash
tensorboard --logdir exp_log/tensorboard --port 6006
```

### ì£¼ìš” ë©”íŠ¸ë¦­
```
Training:
  â€¢ Total loss
  â€¢ Global contrastive loss
  â€¢ Region alignment loss
  â€¢ Learning rate
  â€¢ Gradient norm

Validation:
  â€¢ Imageâ†’Text R@1, R@5, R@10
  â€¢ Textâ†’Image R@1, R@5, R@10
  â€¢ Mean Reciprocal Rank (MRR)
  â€¢ Region alignment accuracy

System:
  â€¢ GPU utilization
  â€¢ GPU memory usage
  â€¢ Data loading time
  â€¢ Training throughput (samples/sec)
```

---

## ğŸ¯ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ë©”ëª¨ë¦¬ ìµœì í™”
- Gradient checkpointing í™œì„±í™”
- Mixed precision (FP16) ì‚¬ìš©
- Batch size ì¡°ì •

### 2. ì†ë„ ìµœì í™”
- ë°ì´í„° ë¡œë” num_workers ì¦ê°€
- Pin memory í™œì„±í™”
- Prefetch factor ì„¤ì •
- NVMe SSD ì‚¬ìš©

### 3. ëª¨ë¸ ìµœì í™”
- Warmup epochsë¡œ ì•ˆì •ì  ì‹œì‘
- Gradient clippingìœ¼ë¡œ ë°œì‚° ë°©ì§€
- Cosine annealingìœ¼ë¡œ ìˆ˜ë ´ ê°œì„ 

---

## ğŸ“ ìš”ì•½

| í•­ëª© | ê°’ |
|------|-----|
| **ëª¨ë¸ëª…** | VLM3D-CT-Encoder |
| **íŒŒë¼ë¯¸í„° ìˆ˜** | ~88M |
| **í›ˆë ¨ ë°ì´í„° ìˆ˜** | 31,007 cases (CT-RATE + OmniAbnorm) |
| **ë°ì´í„° í¬ê¸°** | ~264 GB |
| **Epochs** | 50 (pre-train) + 20 (fine-tune) = 70 |
| **Batch Size** | 8-16 per GPU |
| **ì˜ˆìƒ í›ˆë ¨ ì‹œê°„** | **15-18 hours** (2Ã— H200) |
| **GPU ë©”ëª¨ë¦¬** | ~80 GB per GPU (peak) |
| **Checkpoint í¬ê¸°** | ~1 GB each |
| **ì˜ˆìƒ ì„±ëŠ¥** | R@5: 55-65% (retrieval) |

**í•µì‹¬**: 2ê°œ H200 GPUë¡œ ì•½ **18ì‹œê°„** ë‚´ì— ì „ì²´ í›ˆë ¨ ì™„ë£Œ ê°€ëŠ¥! ğŸš€
