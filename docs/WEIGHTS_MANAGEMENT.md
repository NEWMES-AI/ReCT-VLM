# ê°€ì¤‘ì¹˜ ê´€ë¦¬ ê°€ì´ë“œ

ReCT-VLM ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ì €ì¥, ê³µìœ , ë‹¤ìš´ë¡œë“œ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“¦ ê°€ì¤‘ì¹˜ êµ¬ì¡°

### í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒì¼ë“¤

```
checkpoints/
â”œâ”€â”€ vision_encoder/
â”‚   â”œâ”€â”€ model_epoch_50.pt              # Vision encoder (88M)
â”‚   â”œâ”€â”€ best_model.pt                  # Best checkpoint
â”‚   â””â”€â”€ config.yaml                    # Training config
â”‚
â”œâ”€â”€ classification/
â”‚   â”œâ”€â”€ model_epoch_10.pt              # Classification head (1M)
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ biobert_projectors.pt          # Vision/Text projectors
â”‚
â”œâ”€â”€ localization/
â”‚   â”œâ”€â”€ model_epoch_15.pt              # Localization module (45M)
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â”œâ”€â”€ text_embedder.pt               # BioBERT embeddings
â”‚   â”œâ”€â”€ denoising_transformer.pt       # Denoising module
â”‚   â””â”€â”€ attention_unet.pt              # U-Net module
â”‚
â”œâ”€â”€ report_generator/
â”‚   â”œâ”€â”€ projector_epoch_5.pt           # Vision-to-LLM projector (256M)
â”‚   â”œâ”€â”€ lora_adapters_epoch_5/         # LoRA weights (325M)
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â””â”€â”€ adapter_model.bin
â”‚   â””â”€â”€ best_projector.pt
â”‚
â””â”€â”€ full_model/
    â”œâ”€â”€ model_phase2_epoch_40.pt       # Complete multi-task model
    â”œâ”€â”€ model_phase3_epoch_10.pt       # After RL fine-tuning
    â””â”€â”€ best_model.pt                  # Best overall model

Total size: ~2-3 GB (without LLM base weights)
```

### ì™¸ë¶€ Pre-trained Weights

ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•œ ì™¸ë¶€ ëª¨ë¸ë“¤:

```
external_weights/
â”œâ”€â”€ biobert/
â”‚   â””â”€â”€ Bio_ClinicalBERT/              # 110M, ~440 MB
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ vocab.txt
â”‚
â””â”€â”€ llama/
    â””â”€â”€ Llama-3.1-70B-Instruct/        # 70B, ~38 GB (4-bit)
        â”œâ”€â”€ model-*.safetensors
        â”œâ”€â”€ config.json
        â”œâ”€â”€ tokenizer.json
        â””â”€â”€ tokenizer_config.json
```

## ğŸ¯ HuggingFace Hub ì—…ë¡œë“œ

### 1. Repository ìƒì„±

HuggingFaceì— model repository ìƒì„±:

```bash
# HuggingFace CLI ë¡œê·¸ì¸
huggingface-cli login

# Or Python API
from huggingface_hub import HfApi
api = HfApi()

# Create repository
api.create_repo(
    repo_id="NEWMES-AI/ReCT-VLM-Large",
    repo_type="model",
    private=False
)
```

### 2. ê°€ì¤‘ì¹˜ ì—…ë¡œë“œ

#### ì „ì²´ ëª¨ë¸ ì—…ë¡œë“œ

```python
from huggingface_hub import HfApi
import os

api = HfApi()
repo_id = "NEWMES-AI/ReCT-VLM-Large"

# Upload full model checkpoint
api.upload_file(
    path_or_fileobj="checkpoints/full_model/best_model.pt",
    path_in_repo="pytorch_model.bin",
    repo_id=repo_id,
)

# Upload config
api.upload_file(
    path_or_fileobj="configs/config_large.yaml",
    path_in_repo="config.yaml",
    repo_id=repo_id,
)

# Upload README
api.upload_file(
    path_or_fileobj="README.md",
    path_in_repo="README.md",
    repo_id=repo_id,
)
```

#### ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì—…ë¡œë“œ

```python
# Vision Encoder
api.create_repo("NEWMES-AI/ReCT-VisionEncoder", repo_type="model")
api.upload_file(
    path_or_fileobj="checkpoints/vision_encoder/best_model.pt",
    path_in_repo="pytorch_model.bin",
    repo_id="NEWMES-AI/ReCT-VisionEncoder",
)

# Classification Head
api.create_repo("NEWMES-AI/ReCT-Classifier", repo_type="model")
api.upload_file(
    path_or_fileobj="checkpoints/classification/best_model.pt",
    path_in_repo="pytorch_model.bin",
    repo_id="NEWMES-AI/ReCT-Classifier",
)

# Localization Module
api.create_repo("NEWMES-AI/ReCT-Localizer", repo_type="model")
api.upload_folder(
    folder_path="checkpoints/localization/",
    repo_id="NEWMES-AI/ReCT-Localizer",
)
```

### 3. Model Card ì‘ì„±

HuggingFaceì— ì—…ë¡œë“œí•  `README.md` (Model Card):

```markdown
---
license: apache-2.0
tags:
- medical-imaging
- vision-language
- multi-task-learning
- ct-scan
- radiology
datasets:
- ibrahimhamamci/CT-RATE
language:
- en
metrics:
- accuracy
- f1
- dice
- bleu
library_name: pytorch
---

# ReCT-VLM: Reasoning-Enhanced CT Vision-Language Model

ReCT-VLM is a comprehensive multi-task learning framework for CT image analysis.

## Model Details

- **Model Type**: Multi-task Vision-Language Model
- **Architecture**: 3D Vision Encoder + Multi-label Classifier + Lesion Localizer + Report Generator
- **Parameters**: 70.5B total (460M trainable)
- **Training Data**: CT-RATE dataset (21,907 cases)

## Usage

\`\`\`python
from rect_vlm import ReCTVLM

model = ReCTVLM.from_pretrained("NEWMES-AI/ReCT-VLM-Large")
predictions = model.predict(ct_volume, segmentation_mask)
\`\`\`

## Citation

\`\`\`bibtex
@article{rect-vlm2025,
  title={ReCT-VLM: Reasoning-Enhanced CT Vision-Language Model},
  author={},
  year={2025}
}
\`\`\`
```

## ğŸ“¥ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ

### ì‚¬ìš©ìë¥¼ ìœ„í•œ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

`scripts/download_weights.py`:

```python
#!/usr/bin/env python3
"""Download pre-trained weights from HuggingFace Hub"""

import argparse
from huggingface_hub import snapshot_download, hf_hub_download
import os

# Repository IDs
REPOS = {
    "full": "NEWMES-AI/ReCT-VLM-Large",
    "medium": "NEWMES-AI/ReCT-VLM-Medium",
    "small": "NEWMES-AI/ReCT-VLM-Small",
    "vision": "NEWMES-AI/ReCT-VisionEncoder",
    "classification": "NEWMES-AI/ReCT-Classifier",
    "localization": "NEWMES-AI/ReCT-Localizer",
}

def download_model(model_name: str, output_dir: str = "./checkpoints"):
    """Download model from HuggingFace Hub."""

    if model_name not in REPOS:
        print(f"Unknown model: {model_name}")
        print(f"Available: {list(REPOS.keys())}")
        return

    repo_id = REPOS[model_name]
    print(f"Downloading {model_name} from {repo_id}...")

    # Download entire repository
    local_dir = os.path.join(output_dir, model_name)
    snapshot_download(
        repo_id=repo_id,
        local_dir=local_dir,
        local_dir_use_symlinks=False
    )

    print(f"âœ“ Downloaded to {local_dir}")

def download_all(output_dir: str = "./checkpoints"):
    """Download all available models."""
    for model_name in REPOS.keys():
        download_model(model_name, output_dir)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download ReCT-VLM weights")
    parser.add_argument(
        "--model",
        choices=["all"] + list(REPOS.keys()),
        default="full",
        help="Model to download"
    )
    parser.add_argument(
        "--output-dir",
        default="./checkpoints",
        help="Output directory"
    )

    args = parser.parse_args()

    if args.model == "all":
        download_all(args.output_dir)
    else:
        download_model(args.model, args.output_dir)
```

### ì‚¬ìš© ì˜ˆì œ

```bash
# ì „ì²´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Large)
python scripts/download_weights.py --model full

# íŠ¹ì • ì»´í¬ë„ŒíŠ¸ë§Œ ë‹¤ìš´ë¡œë“œ
python scripts/download_weights.py --model vision
python scripts/download_weights.py --model classification

# Medium ëª¨ë¸
python scripts/download_weights.py --model medium

# ëª¨ë“  ëª¨ë¸
python scripts/download_weights.py --model all
```

## ğŸ”„ ë²„ì „ ê´€ë¦¬

### ê°€ì¤‘ì¹˜ ë²„ì €ë‹

```
ReCT-VLM-Large/
â”œâ”€â”€ v0.1.0/                 # Initial release
â”‚   â””â”€â”€ pytorch_model.bin
â”œâ”€â”€ v0.2.0/                 # Improved after RL
â”‚   â””â”€â”€ pytorch_model.bin
â””â”€â”€ main/                   # Latest stable
    â””â”€â”€ pytorch_model.bin
```

### Git Tags ì‚¬ìš©

```bash
# Tag checkpoint
git tag -a weights-v0.1.0 -m "Release v0.1.0 weights"
git push origin weights-v0.1.0

# Download specific version
from huggingface_hub import hf_hub_download

hf_hub_download(
    repo_id="NEWMES-AI/ReCT-VLM-Large",
    filename="pytorch_model.bin",
    revision="v0.1.0"  # Specific version
)
```

## ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í˜•ì‹

### PyTorch í˜•ì‹

```python
# ì €ì¥
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'best_metric': best_metric,
    'config': config,
}
torch.save(checkpoint, 'checkpoint.pt')

# ë¡œë“œ
checkpoint = torch.load('checkpoint.pt')
model.load_state_dict(checkpoint['model_state_dict'])
```

### SafeTensors í˜•ì‹ (ê¶Œì¥)

```python
from safetensors.torch import save_file, load_file

# ì €ì¥
save_file(model.state_dict(), 'model.safetensors')

# ë¡œë“œ
state_dict = load_file('model.safetensors')
model.load_state_dict(state_dict)
```

SafeTensorsì˜ ì¥ì :
- ë” ì•ˆì „ (ì„ì˜ ì½”ë“œ ì‹¤í–‰ ë¶ˆê°€)
- ë” ë¹ ë¥¸ ë¡œë”©
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- í”„ë ˆì„ì›Œí¬ ê°„ í˜¸í™˜ì„±

## ğŸŒ ê³µê°œ vs ë¹„ê³µê°œ

### ê³µê°œ (Public)

**ì¥ì **:
- ì—°êµ¬ ì¬í˜„ì„±
- ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬
- Citation ì¦ê°€

**ê¶Œì¥ ì‹œê¸°**:
- ë…¼ë¬¸ accept í›„
- ì¶©ë¶„í•œ ê²€ì¦ ì™„ë£Œ

### ë¹„ê³µê°œ (Private)

**ì‚¬ìš© ì‹œê¸°**:
- ë…¼ë¬¸ ì œì¶œ ì¤‘
- ì¶”ê°€ ì‹¤í—˜ ì§„í–‰ ì¤‘
- ìƒì—…ì  ì‚¬ìš© ê³ ë ¤

```python
# Private repository ìƒì„±
api.create_repo(
    repo_id="NEWMES-AI/ReCT-VLM-Private",
    repo_type="model",
    private=True  # Private
)
```

## ğŸ“Š ê°€ì¤‘ì¹˜ í¬ê¸° ìµœì í™”

### ì–‘ìí™” (Quantization)

```python
# 4-bit quantization
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# 70B â†’ ~35 GB
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-70B-Instruct",
    quantization_config=quantization_config
)
```

### ëª¨ë¸ í”„ë£¨ë‹

```python
import torch.nn.utils.prune as prune

# Prune 20% of weights
prune.l1_unstructured(module, name="weight", amount=0.2)
```

## ğŸ” ì ‘ê·¼ ì œì–´

### Gated Model

ë…¼ë¬¸ accept ì „ê¹Œì§€ gated access:

```python
# HuggingFace Model Cardì— ì¶”ê°€
---
extra_gated_prompt: "Request access to ReCT-VLM weights"
extra_gated_fields:
  Name: text
  Email: text
  Organization: text
  Purpose: text
---
```

### API Token

```bash
# User needs HF token to download
huggingface-cli login

# In code
from huggingface_hub import hf_hub_download

hf_hub_download(
    repo_id="NEWMES-AI/ReCT-VLM-Large",
    filename="pytorch_model.bin",
    use_auth_token=True  # Requires login
)
```

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì—…ë¡œë“œ ì „ í™•ì¸:

- [ ] ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
- [ ] ë¯¼ê° ì •ë³´ ì œê±° (paths, tokens, etc.)
- [ ] Model Card ì‘ì„± ì™„ë£Œ
- [ ] LICENSE í¬í•¨
- [ ] ì‚¬ìš© ì˜ˆì œ í¬í•¨
- [ ] ë²„ì „ íƒœê·¸ ì§€ì •
- [ ] íŒŒì¼ í¬ê¸° ìµœì í™”

ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸:

- [ ] ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‘ë™ í™•ì¸
- [ ] ë¡œë“œ í›„ inference í…ŒìŠ¤íŠ¸
- [ ] ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸

## ğŸ‰ ì™„ë£Œ

ê°€ì¤‘ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ë©´:

1. **HuggingFace Hub**: https://huggingface.co/NEWMES-AI
2. **Model Zoo**: README.mdì— ë§í¬ ì¶”ê°€
3. **Documentation**: ë‹¤ìš´ë¡œë“œ ë°©ë²• ë¬¸ì„œí™”
4. **Announcement**: ì»¤ë®¤ë‹ˆí‹°ì— ê³µì§€

---

**ë¬¸ì˜**: GitHub Issues ë˜ëŠ” ì´ë©”ì¼
