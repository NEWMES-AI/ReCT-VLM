# ReCT-VLM: Reasoning-Enhanced CT Vision-Language Model

[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10%2B-green.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org/)

Official implementation of **ReCT-VLM: Reasoning-Enhanced CT Vision-Language Model for Multi-task Medical Image Understanding**.

> **ReCT-VLM** is a comprehensive multi-task learning framework for CT image analysis that performs:
> - **Multi-label Disease Classification** (18 thoracic diseases)
> - **Lesion Localization** (5 key diseases with 3-stage text-guided pipeline)
> - **Clinical Report Generation** (LLM-based with vision-language alignment)

**Key Features**:
- üî¨ Native 3D processing with slice-aware and region-aware attention
- üìù LLM-based report generation with LoRA fine-tuning
- üéØ Text-guided lesion localization with anatomical context
- üöÄ Multi-task learning with shared 3D vision encoder
- üìä Achieves SOTA performance on CT-RATE dataset

---

## üèóÔ∏è Architecture Overview

<p align="center">
  <img src="docs/images/architecture.png" alt="ReCT-VLM Architecture" width="800"/>
</p>

ReCT-VLM consists of four main components:

1. **3D Vision Encoder** (88M params)
   - Native 3D processing with ViT-based architecture
   - Slice-aware attention for volumetric context
   - Region-aware attention for anatomical structure integration

2. **Multi-label Classifier** (1M trainable)
   - Text-prompt based classification using BioBERT
   - 18 thoracic disease predictions
   - Cosine similarity with learnable temperature

3. **Lesion Localization Module** (45M params)
   - **Stage 1**: Disease text embedding (BioBERT)
   - **Stage 2**: Denoising transformer (4 layers)
   - **Stage 3**: Text-guided Attention U-Net
   - 5 key diseases: pericardial effusion, pleural effusion, consolidation, ground-glass opacity, lung nodules

4. **Report Generator** (325M trainable via LoRA)
   - Vision-to-LLM projector (Q-Former style)
   - Llama-3.1-70B with LoRA adapters
   - Lesion location injection
   - Optional RL fine-tuning (GRPO)

**Total**: ~70.5B parameters (460M trainable, 0.65%)

---

## üöÄ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/NEWMES-AI/ReCT-VLM.git
cd ReCT-VLM

# Create conda environment
conda create -n rect-vlm python=3.10
conda activate rect-vlm

# Install dependencies
pip install -r requirements.txt

# Install from source
pip install -e .
```

### Download Pre-trained Weights

```bash
# Download all pre-trained weights
python scripts/download_weights.py --all

# Or download specific components
python scripts/download_weights.py --vision-encoder
python scripts/download_weights.py --full-model
```

Pre-trained weights are hosted on [HuggingFace Hub](https://huggingface.co/NEWMES-AI/ReCT-VLM).

### Quick Inference

```python
from rect_vlm import ReCTVLM
import nibabel as nib

# Load model
model = ReCTVLM.from_pretrained("NEWMES-AI/ReCT-VLM-Large")
model = model.cuda()

# Load CT volume
ct_volume = nib.load("path/to/ct_scan.nii.gz").get_fdata()
seg_mask = nib.load("path/to/seg_mask.nii.gz").get_fdata()

# Inference
predictions = model.predict(
    ct_volume=ct_volume,
    segmentation_mask=seg_mask,
    generate_report=True
)

# Results
print("Diseases detected:", predictions['classification']['disease_names'])
print("Lesion masks:", predictions['localization']['masks'].keys())
print("\nGenerated Report:")
print(predictions['reports'][0])
```

---

## üìä Dataset Preparation

### CT-RATE Dataset

We use the [CT-RATE dataset](https://huggingface.co/datasets/ibrahimhamamci/CT-RATE) from HuggingFace.

```bash
# 1. Download CT-RATE dataset
cd DATA
python download_dataset.py

# 2. Prepare data for training
python prepare_ctrate_for_medsam2.py \
    --volume-dir ./CT-RATE/lung_nodule/volume \
    --mask-dir ./CT-RATE/lung_nodule/masks \
    --output-dir ./CT-RATE/lung_nodule_medsam2 \
    --split all
```

**Dataset Statistics**:
- Training samples: 21,907 CT volumes
- Validation samples: 1,000 CT volumes
- 18 disease labels (multi-label)
- 5 lesion segmentation masks
- Total size: ~275 GB (compressed NPZ format)

See [DATA/README.md](DATA/README.md) for detailed instructions.

### OmniAbnorm CoT Dataset

Chain-of-Thought reasoning data generated using Llama-3.1-70B:

```bash
cd DATA
python generate_cot_data.py \
    --input-dir ./OmniAbnorm \
    --output-dir ./OmniAbnorm/cot_data \
    --model-name meta-llama/Llama-3.1-70B-Instruct
```

**Statistics**:
- Total samples: 10,117 cases
- Format: JSON with detailed reasoning chains
- Size: ~1 GB

---

## üéì Training

### Phase 1: Task-Specific Pre-training

Train each task individually for better initialization:

```bash
# 1.1. Classification only (10 epochs, ~30 min)
python training/train_multitask.py \
    --config configs/config_large.yaml \
    --phase 1.1 \
    --tasks classification \
    --epochs 10 \
    --batch-size 16 \
    --output-dir ./exp_log/phase1_cls

# 1.2. Localization only (15 epochs, ~2-3 hrs)
python training/train_multitask.py \
    --config configs/config_large.yaml \
    --phase 1.2 \
    --tasks localization \
    --epochs 15 \
    --batch-size 4 \
    --output-dir ./exp_log/phase1_loc

# 1.3. Report generation alignment (5 epochs, ~1-1.5 hrs)
python training/train_multitask.py \
    --config configs/config_large.yaml \
    --phase 1.3 \
    --tasks generation \
    --epochs 5 \
    --batch-size 4 \
    --freeze-vision \
    --output-dir ./exp_log/phase1_gen
```

### Phase 2: Joint Multi-task Training

Train all tasks simultaneously with shared vision encoder:

```bash
python training/train_multitask.py \
    --config configs/config_large.yaml \
    --phase 2 \
    --tasks classification localization generation \
    --epochs 40 \
    --batch-size 4 \
    --load-phase1-checkpoints \
        ./exp_log/phase1_cls/best_model.pt \
        ./exp_log/phase1_loc/best_model.pt \
        ./exp_log/phase1_gen/best_model.pt \
    --output-dir ./exp_log/phase2_joint
```

**Training time**: ~12-15 hours on 2√ó NVIDIA H200 (80GB)

### Phase 3: RL Fine-tuning for Reports (Optional)

Improve report quality using reinforcement learning:

```bash
python training/train_rl_reports.py \
    --config configs/config_large.yaml \
    --phase 3 \
    --method grpo \
    --epochs 10 \
    --batch-size 2 \
    --num-generations 4 \
    --checkpoint ./exp_log/phase2_joint/best_model.pt \
    --output-dir ./exp_log/phase3_rl
```

**Training time**: ~12-15 hours (can be reduced to 4-5 hours with optimizations)

**Total training time**: ~24-28 hours

### Monitoring Training

```bash
# TensorBoard
tensorboard --logdir ./exp_log/phase2_joint/tensorboard

# Watch GPU usage
watch -n 1 nvidia-smi
```

See [docs/TRAINING.md](docs/TRAINING.md) for detailed training instructions.

---

## üìà Evaluation

### Evaluate on Validation Set

```bash
python scripts/evaluate.py \
    --checkpoint ./exp_log/phase2_joint/best_model.pt \
    --data-dir ./DATA/CT-RATE/lung_nodule_medsam2 \
    --split valid \
    --output-dir ./evaluation_results \
    --generate-reports \
    --save-visualizations
```

### Expected Performance

| Task | Metric | Zero-shot | After Training | Improvement |
|------|--------|-----------|----------------|-------------|
| **Classification** | AUC-ROC (macro) | 0.50-0.60 | **0.85-0.92** | +35-50% |
| | F1 (macro) | 0.40-0.50 | **0.75-0.85** | +35-45% |
| **Localization** | Dice (mean) | 0.20-0.35 | **0.65-0.80** | +45-60% |
| | IoU (mean) | 0.15-0.25 | **0.55-0.70** | +40-55% |
| **Report Generation** | BLEU-4 | 0.05-0.10 | **0.30-0.40** | +20-35% |
| | ROUGE-L | 0.15-0.25 | **0.45-0.55** | +30-40% |

---

## üíæ Model Zoo

We provide multiple model configurations for different resource requirements:

| Model | LLM Backbone | Total Params | Trainable | VRAM | Download |
|-------|--------------|--------------|-----------|------|----------|
| **ReCT-VLM-Large** | Llama-3.1-70B | 70.5B | 460M | ~80 GB | [HF Hub](https://huggingface.co/NEWMES-AI/ReCT-VLM-Large) |
| **ReCT-VLM-Medium** | Gemma-2-27B | 27.3B | 280M | ~50 GB | [HF Hub](https://huggingface.co/NEWMES-AI/ReCT-VLM-Medium) |
| **ReCT-VLM-Small** | Llama-3.1-8B | 8.3B | 120M | ~25 GB | [HF Hub](https://huggingface.co/NEWMES-AI/ReCT-VLM-Small) |

**Pre-trained Components** (can be used independently):
- [Vision Encoder Only](https://huggingface.co/NEWMES-AI/ReCT-VisionEncoder) (88M params)
- [Classification Head](https://huggingface.co/NEWMES-AI/ReCT-Classifier) (1M params)
- [Localization Module](https://huggingface.co/NEWMES-AI/ReCT-Localizer) (45M params)

---

## üìÅ Repository Structure

```
ReCT-VLM/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ LICENSE                            # Apache 2.0 License
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ setup.py                           # Package installation
‚îÇ
‚îú‚îÄ‚îÄ rect_vlm/                          # Main package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model/                         # Model implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vision_encoder.py         # 3D Vision Encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification_head.py    # Multi-label Classifier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ localization_module.py    # 3-stage Localization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_generator.py       # LLM-based Report Generator
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multi_task_model.py       # Unified Multi-task Model
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/                      # Training infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset_multitask.py      # Dataset loader
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_multitask.py        # Training script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                # Evaluation metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                # Trainer class
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                         # Utility functions
‚îÇ       ‚îú‚îÄ‚îÄ data_utils.py
‚îÇ       ‚îú‚îÄ‚îÄ visualization.py
‚îÇ       ‚îî‚îÄ‚îÄ checkpoint_utils.py
‚îÇ
‚îú‚îÄ‚îÄ configs/                           # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ config_large.yaml             # Large model (Llama-70B)
‚îÇ   ‚îú‚îÄ‚îÄ config_medium.yaml            # Medium model (Gemma-27B)
‚îÇ   ‚îî‚îÄ‚îÄ config_small.yaml             # Small model (Llama-8B)
‚îÇ
‚îú‚îÄ‚îÄ scripts/                           # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ download_weights.py           # Download pre-trained weights
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                   # Evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ inference.py                  # Inference script
‚îÇ   ‚îî‚îÄ‚îÄ visualize_results.py          # Visualization script
‚îÇ
‚îú‚îÄ‚îÄ DATA/                              # Dataset preparation
‚îÇ   ‚îú‚îÄ‚îÄ README.md                     # Dataset documentation
‚îÇ   ‚îú‚îÄ‚îÄ download_dataset.py           # Download CT-RATE
‚îÇ   ‚îú‚îÄ‚îÄ prepare_ctrate_for_medsam2.py # Data preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ generate_cot_data.py          # Generate CoT data
‚îÇ
‚îú‚îÄ‚îÄ docs/                              # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md               # Architecture details
‚îÇ   ‚îú‚îÄ‚îÄ TRAINING.md                   # Training guide
‚îÇ   ‚îú‚îÄ‚îÄ EVALUATION.md                 # Evaluation guide
‚îÇ   ‚îú‚îÄ‚îÄ MODULE_TRAINING_DETAILS.md    # Per-module training info
‚îÇ   ‚îî‚îÄ‚îÄ images/                       # Documentation images
‚îÇ
‚îú‚îÄ‚îÄ examples/                          # Example notebooks
‚îÇ   ‚îú‚îÄ‚îÄ inference_example.ipynb       # Inference tutorial
‚îÇ   ‚îú‚îÄ‚îÄ training_example.ipynb        # Training tutorial
‚îÇ   ‚îî‚îÄ‚îÄ visualization_example.ipynb   # Visualization tutorial
‚îÇ
‚îî‚îÄ‚îÄ tests/                             # Unit tests
    ‚îú‚îÄ‚îÄ test_vision_encoder.py
    ‚îú‚îÄ‚îÄ test_classification.py
    ‚îú‚îÄ‚îÄ test_localization.py
    ‚îî‚îÄ‚îÄ test_report_generation.py
```

---

## üõ†Ô∏è Requirements

### Hardware Requirements

**Minimum**:
- GPUs: 2√ó NVIDIA H200 (80GB VRAM)
- RAM: 256 GB
- Storage: 500 GB SSD

**Recommended**:
- GPUs: 4√ó NVIDIA H200 (80GB VRAM)
- RAM: 512 GB
- Storage: 1 TB NVMe SSD

### Software Requirements

- Python >= 3.10
- PyTorch >= 2.0
- CUDA >= 12.1
- transformers >= 4.35.0
- See [requirements.txt](requirements.txt) for complete list

---

## üìñ Documentation

- **[Architecture Details](docs/ARCHITECTURE.md)**: Detailed explanation of model architecture
- **[Training Guide](docs/TRAINING.md)**: Step-by-step training instructions
- **[Evaluation Guide](docs/EVALUATION.md)**: Evaluation metrics and benchmarks
- **[Module Training Details](docs/MODULE_TRAINING_DETAILS.md)**: Per-module training information
- **[API Documentation](docs/API.md)**: Python API reference

---

## üéØ Use Cases

### 1. Disease Classification
```python
from rect_vlm import ReCTVLM

model = ReCTVLM.from_pretrained("NEWMES-AI/ReCT-VLM-Large")
predictions = model.classify(ct_volume, threshold=0.5)
print("Detected diseases:", predictions['disease_names'])
```

### 2. Lesion Localization
```python
lesion_masks = model.localize_lesions(ct_volume, segmentation_mask)
for disease, mask in lesion_masks.items():
    print(f"{disease}: {mask.sum()} voxels")
```

### 3. Report Generation
```python
report = model.generate_report(
    ct_volume,
    segmentation_mask,
    max_new_tokens=512,
    temperature=0.7
)
print(report)
```

### 4. End-to-End Analysis
```python
results = model.predict(
    ct_volume=ct_volume,
    segmentation_mask=seg_mask,
    generate_report=True,
    classification_threshold=0.5,
    localization_threshold=0.5
)

# Get all outputs
diseases = results['classification']['disease_names']
lesions = results['localization']['masks']
report = results['reports'][0]
```

---

## üî¨ Research and Citation

If you use ReCT-VLM in your research, please cite:

```bibtex
@article{rect-vlm2025,
  title={ReCT-VLM: Reasoning-Enhanced CT Vision-Language Model for Multi-task Medical Image Understanding},
  author={},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
```

---

## ü§ù Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**How to contribute**:
1. Fork the repository
2. Create a new branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìú License

This project is licensed under the Apache License 2.0 - see [LICENSE](LICENSE) file for details.

**Pre-trained Model Licenses**:
- BioBERT: [MIT License](https://github.com/dmis-lab/biobert)
- Llama-3.1: [Llama 3.1 Community License](https://ai.meta.com/llama/license/)
- Gemma-2: [Gemma Terms of Use](https://ai.google.dev/gemma/terms)

---

## üôè Acknowledgements

This work is based on several excellent open-source projects:

- [CT-RATE Dataset](https://huggingface.co/datasets/ibrahimhamamci/CT-RATE) by Ibrahim Hamamci et al.
- [MedSAM2](https://github.com/bowang-lab/MedSAM) for medical image segmentation
- [BioBERT](https://github.com/dmis-lab/biobert) for medical text encoding
- [Llama-3.1](https://ai.meta.com/llama/) by Meta AI
- [HuggingFace Transformers](https://github.com/huggingface/transformers)

---

## üìß Contact

For questions and feedback:
- **Issues**: [GitHub Issues](https://github.com/NEWMES-AI/ReCT-VLM/issues)
- **Email**: [contact email]
- **Website**: [project website]

---

## üó∫Ô∏è Roadmap

- [x] Initial code release
- [x] Pre-trained weights release
- [ ] Paper submission to MICCAI 2025
- [ ] Demo application
- [ ] Multi-language support
- [ ] Integration with clinical systems
- [ ] Model distillation for deployment
- [ ] Web API service

---

**‚≠ê If you find this project useful, please consider giving it a star!**

---

<p align="center">
  Made with ‚ù§Ô∏è by NEWMES-AI
</p>
